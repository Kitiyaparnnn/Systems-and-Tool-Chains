{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question1\n",
    "\n",
    "use TensorFlow to build neural networks to predict a multi-class classification problem with 5 possible categories (normal, DOS,\n",
    "R2L, U2R, probing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.1\n",
    "\n",
    "Prepare the data and convert them to appropriate Tensor formats needed\n",
    "for TensorFlow. You may use KDDTrain+.txt as the training dataset, 50% of\n",
    "KDDTest+.txt as the validation dataset, and the remaining 50% as the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.ml import Pipeline,Transformer\n",
    "from pyspark.ml.feature import Imputer,StandardScaler,StringIndexer,OneHotEncoder, VectorAssembler\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import numpy as np\n",
    "\n",
    "col_names = [\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n",
    "\"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
    "\"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
    "\"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
    "\"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
    "\"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
    "\"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
    "\"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
    "\"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
    "\"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"classes\",\"difficulty\"]\n",
    "\n",
    "nominal_cols = ['protocol_type','service','flag']\n",
    "binary_cols = ['land', 'logged_in', 'root_shell', 'su_attempted', 'is_host_login',\n",
    "'is_guest_login']\n",
    "continuous_cols = ['duration' ,'src_bytes', 'dst_bytes', 'wrong_fragment' ,'urgent', 'hot',\n",
    "'num_failed_logins', 'num_compromised', 'num_root' ,'num_file_creations',\n",
    "'num_shells', 'num_access_files', 'num_outbound_cmds', 'count' ,'srv_count',\n",
    "'serror_rate', 'srv_serror_rate' ,'rerror_rate' ,'srv_rerror_rate',\n",
    "'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate' ,'dst_host_count',\n",
    "'dst_host_srv_count' ,'dst_host_same_srv_rate' ,'dst_host_diff_srv_rate',\n",
    "'dst_host_same_src_port_rate' ,'dst_host_srv_diff_host_rate',\n",
    "'dst_host_serror_rate' ,'dst_host_srv_serror_rate', 'dst_host_rerror_rate',\n",
    "'dst_host_srv_rerror_rate']\n",
    "\n",
    "class OutcomeCreater(Transformer): # this defines a transformer that creates the outcome column\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        def attack_category(attack_type):\n",
    "            if attack_type == 'normal':\n",
    "                return 0\n",
    "            elif attack_type in ['port-Sweep', 'ip-Sweep', 'nmap', 'satan', 'saint', 'mscan']:\n",
    "                return 1  # Probing\n",
    "            elif attack_type in ['neptune', 'smurf', 'pod', 'teardrop', 'land', 'back', 'apache2',\n",
    "                                'udpstorm', 'processtable', 'mail-Bomb']:\n",
    "                return 2  # Dos\n",
    "            elif attack_type in ['buffer-Overflow', 'load-Module', 'perl', 'rootkit', 'xterm',\n",
    "                                'ps', 'sqlattack']:\n",
    "                return 3  # U2R\n",
    "            else:\n",
    "                return 4  # R2L\n",
    "          \n",
    "        # Convert the function to a UDF, specifying IntegerType for output\n",
    "        label_to_multiclasses = udf(attack_category, IntegerType())\n",
    "        output_df = dataset.withColumn('outcomes', label_to_multiclasses(col('classes'))).drop(\"classes\")  \n",
    "        output_df = output_df.withColumn('outcomes', col('outcomes').cast(DoubleType()))\n",
    "        output_df = output_df.drop('difficulty_level')\n",
    "        return output_df\n",
    "\n",
    "class FeatureTypeCaster(Transformer): # this transformer will cast the columns as appropriate types  \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        output_df = dataset\n",
    "        for col_name in binary_cols + continuous_cols:\n",
    "            output_df = output_df.withColumn(col_name,col(col_name).cast(DoubleType()))\n",
    "\n",
    "        return output_df\n",
    "class ColumnDropper(Transformer): # this transformer drops unnecessary columns\n",
    "    def __init__(self, columns_to_drop = None):\n",
    "        super().__init__()\n",
    "        self.columns_to_drop=columns_to_drop\n",
    "    def _transform(self, dataset):\n",
    "        output_df = dataset\n",
    "        for col_name in self.columns_to_drop:\n",
    "            output_df = output_df.drop(col_name)\n",
    "        return output_df\n",
    "\n",
    "def get_preprocess_pipeline():\n",
    "    # Stage where columns are casted as appropriate types\n",
    "    stage_typecaster = FeatureTypeCaster()\n",
    "\n",
    "    # Stage where nominal columns are transformed to index columns using StringIndexer\n",
    "    nominal_id_cols = [x+\"_index\" for x in nominal_cols]\n",
    "    nominal_onehot_cols = [x+\"_encoded\" for x in nominal_cols]\n",
    "    stage_nominal_indexer = StringIndexer(inputCols = nominal_cols, outputCols = nominal_id_cols )\n",
    "\n",
    "    # Stage where the index columns are further transformed using OneHotEncoder\n",
    "    stage_nominal_onehot_encoder = OneHotEncoder(inputCols=nominal_id_cols, outputCols=nominal_onehot_cols)\n",
    "\n",
    "    # Stage where all relevant features are assembled into a vector (and dropping a few)\n",
    "    feature_cols = continuous_cols+binary_cols+nominal_onehot_cols\n",
    "    corelated_cols_to_remove = [\"dst_host_serror_rate\",\"srv_serror_rate\",\"dst_host_srv_serror_rate\",\n",
    "                     \"srv_rerror_rate\",\"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\"]\n",
    "    for col_name in corelated_cols_to_remove:\n",
    "        feature_cols.remove(col_name)\n",
    "    stage_vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"vectorized_features\")\n",
    "\n",
    "    # Stage where we scale the columns\n",
    "    stage_scaler = StandardScaler(inputCol= 'vectorized_features', outputCol= 'features')\n",
    "    \n",
    "    # Stage for creating the outcome column representing whether there is attack \n",
    "    stage_outcome = OutcomeCreater()\n",
    "\n",
    "    # Removing all unnecessary columbs, only keeping the 'features' and 'outcome' columns\n",
    "    stage_column_dropper = ColumnDropper(columns_to_drop = nominal_cols+nominal_id_cols+\n",
    "        nominal_onehot_cols+ binary_cols + continuous_cols + ['vectorized_features'])\n",
    "    \n",
    "    # Connect the columns into a pipeline\n",
    "    pipeline = Pipeline(stages=[stage_typecaster,stage_nominal_indexer,stage_nominal_onehot_encoder,\n",
    "        stage_vector_assembler,stage_scaler,stage_outcome,stage_column_dropper])\n",
    "    return pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset:  125973\n",
      "test dataset:  22544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "# Contribute Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Pytorch\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load train and test data\n",
    "nslkdd_raw = spark.read.csv('/Users/kitiya/Documents/CMU@2024/14763 Systems&ToolChain/hw6/hw6-pytorch-Kitiyaparnnn/NSL-KDD/KDDTrain+.txt',header=False).toDF(*col_names)\n",
    "nslkdd_test_raw = spark.read.csv('/Users/kitiya/Documents/CMU@2024/14763 Systems&ToolChain/hw6/hw6-pytorch-Kitiyaparnnn/NSL-KDD/KDDTest+.txt',header=False).toDF(*col_names)\n",
    "print(\"train dataset: \", nslkdd_raw.count())\n",
    "print(\"test dataset: \", nslkdd_test_raw.count())\n",
    "\n",
    "preprocess_pipeline = get_preprocess_pipeline()\n",
    "preprocess_pipeline_model = preprocess_pipeline.fit(nslkdd_raw)\n",
    "\n",
    "nslkdd_df = preprocess_pipeline_model.transform(nslkdd_raw)\n",
    "nslkdd_df_test = preprocess_pipeline_model.transform(nslkdd_test_raw)\n",
    "\n",
    "\n",
    "nslkdd_df.cache()\n",
    "nslkdd_df_test.cache()\n",
    "\n",
    "to_array = udf(lambda v: v.toArray().tolist(), ArrayType(FloatType()))\n",
    "\n",
    "# Split test data into validation dataset and test dataset\n",
    "val_df, test_df = nslkdd_df_test.randomSplit(weights=[0.5,0.5], seed=100)\n",
    "\n",
    "# Convert Spark to Pandas\n",
    "train_df_pandas = nslkdd_df.withColumn('features',to_array('features')).toPandas()\n",
    "val_df_pandas = val_df.withColumn('features',to_array('features')).toPandas()\n",
    "test_df_pandas = test_df.withColumn('features',to_array('features')).toPandas()\n",
    "\n",
    "# Convert Pandas to Tensor\n",
    "x_train = tf.constant(np.array(train_df_pandas['features'].values.tolist()))\n",
    "y_train = tf.constant(np.array(train_df_pandas['outcomes'].values.tolist()))\n",
    "\n",
    "x_val = tf.constant(np.array(val_df_pandas['features'].values.tolist()))\n",
    "y_val = tf.constant(np.array(val_df_pandas['outcomes'].values.tolist()))\n",
    "\n",
    "x_test = tf.constant(np.array(test_df_pandas['features'].values.tolist()))\n",
    "y_test = tf.constant(np.array(test_df_pandas['outcomes'].values.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.2\n",
    "\n",
    " Build a Neural Network using tf.keras, and conduct training and validation.\n",
    "Select the appropriate loss function and choose at least one metric. Set the appropriate\n",
    "number of epochs. After training, evaluate your trained model on the test data.\n",
    "\n",
    "In your submission, include a screenshot of the output of the fit function and the\n",
    "evaluate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 13:27:29.379510: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-11-17 13:27:53.007837: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3937/3937 - 26s - loss: 0.1437 - Accuracy: 1.0000 - val_loss: 2.2816 - val_Accuracy: 1.0000 - 26s/epoch - 7ms/step\n",
      "Epoch 2/5\n",
      "3937/3937 - 25s - loss: 0.0993 - Accuracy: 1.0000 - val_loss: 2.3989 - val_Accuracy: 1.0000 - 25s/epoch - 6ms/step\n",
      "Epoch 3/5\n",
      "3937/3937 - 26s - loss: 0.0967 - Accuracy: 1.0000 - val_loss: 2.5542 - val_Accuracy: 1.0000 - 26s/epoch - 7ms/step\n",
      "Epoch 4/5\n",
      "3937/3937 - 26s - loss: 0.1007 - Accuracy: 1.0000 - val_loss: 2.5032 - val_Accuracy: 1.0000 - 26s/epoch - 7ms/step\n",
      "Epoch 5/5\n",
      "3937/3937 - 25s - loss: 0.1020 - Accuracy: 1.0000 - val_loss: 2.8751 - val_Accuracy: 1.0000 - 25s/epoch - 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x376fb3fa0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# Create a tensorflow model\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(5, activation='relu'),\n",
    "        keras.layers.Dense(5)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.05), \n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[keras.metrics.SparseTopKCategoricalAccuracy(name='Accuracy')])\n",
    "\n",
    "# tensorboard\n",
    "log_dir = \"logs/multiClasses/\"+ datetime.datetime.now().strftime(\"%d-%m-%Y-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model.fit(x=x_train, y=y_train, epochs=5, verbose=2,\n",
    "          validation_data=(x_val,y_val),\n",
    "          callbacks=[tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test set:\n",
      "351/351 - 2s - loss: 2.8747 - Accuracy: 1.0000 - 2s/epoch - 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.8747165203094482, 1.0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "print(\"Evaluate on test set:\")\n",
    "model.evaluate(x_test,y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_18 (Dense)            (None, 5)                 570       \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 5)                 30        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 600 (2.34 KB)\n",
      "Trainable params: 600 (2.34 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to q1-2_1.png, q1-2_2.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.3\n",
    "\n",
    "Display the results in TensorBoard. In your submission, include screenshots\n",
    "of the loss and the metrics for both the training and the validation run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run in the terminal**\n",
    "\n",
    "`load_ext tensorboard`\n",
    "\n",
    "`tensorboard --logdir logs/multiClasses/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to q1-3_1.png, q1-3_2.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Reference*\n",
    "\n",
    "- Guannan Qu. (2024). Lecture 17: TensorFlow sildes.\n",
    "- homework6.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
