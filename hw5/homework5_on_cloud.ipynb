{"cells":[{"cell_type":"markdown","id":"e6d3a956","metadata":{},"source":["#### Load dataset"]},{"cell_type":"code","execution_count":1,"id":"1bca914a","metadata":{"scrolled":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","24/10/14 22:20:41 INFO SparkEnv: Registering MapOutputTracker\n","24/10/14 22:20:42 INFO SparkEnv: Registering BlockManagerMaster\n","24/10/14 22:20:42 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n","24/10/14 22:20:42 INFO SparkEnv: Registering OutputCommitCoordinator\n"]}],"source":["import pyspark\n","from pyspark.sql import SparkSession\n","from pyspark import SparkContext, SQLContext\n","from pyspark.ml import Pipeline,Transformer\n","from pyspark.ml.feature import Imputer,StandardScaler,StringIndexer,OneHotEncoder, VectorAssembler\n","from pyspark.ml.classification import LogisticRegression\n","\n","from pyspark.sql.functions import *\n","from pyspark.sql.types import *\n","import numpy as np\n","\n","appName = \"SparkML-test\"\n","master = \"yarn\"\n","\n","# # Create Configuration object for Spark.\n","# conf = pyspark.SparkConf()\\\n","# .set('spark.driver.host','127.0.0.1')\\\n","# .setAppName(appName)\\\n","# .setMaster(master)\n","\n","# # Create Spark Context \n","# sc = SparkContext.getOrCreate(conf=conf)\n","\n","# # Create SQL Context to conduct some database operations\n","# sqlContext = SQLContext(sc)\n","\n","# # If you have SQL context, you create the session from the Spark Context\n","# spark = sqlContext.sparkSession.builder\\\n","# .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.2.29\") \\\n","# .config(\"spark.sql.execution.arrow.enabled\",\"true\")\\\n","# .getOrCreate() \n","\n","spark = SparkSession.builder \\\n","        .master(master) \\\n","        .appName(appName) \\\n","        .getOrCreate()\n","        "]},{"cell_type":"code","execution_count":2,"id":"25efd429","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["col_names = [\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n","\"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n","\"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n","\"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n","\"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n","\"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n","\"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n","\"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n","\"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n","\"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"class\",\"difficulty\"]\n","\n","# Split train and test dataset\n","data_path = 'gs://kdd-dataset/'\n","\n","nslkdd_raw = spark.read.csv(data_path + 'KDDTrain+.txt',header=False).toDF(*col_names)\n","nslkdd_test_raw = spark.read.csv(data_path + 'KDDTest+.txt',header=False).toDF(*col_names)"]},{"cell_type":"markdown","id":"3d175860","metadata":{},"source":["#### Data preprocessing"]},{"cell_type":"code","execution_count":3,"id":"b5db3278","metadata":{},"outputs":[],"source":["from pyspark.sql.types import IntegerType\n","\n","nominal_cols = ['protocol_type','service','flag']\n","binary_cols = ['land', 'logged_in', 'root_shell', 'su_attempted', 'is_host_login',\n","'is_guest_login']\n","continuous_cols = ['duration' ,'src_bytes', 'dst_bytes', 'wrong_fragment' ,'urgent', 'hot',\n","'num_failed_logins', 'num_compromised', 'num_root' ,'num_file_creations',\n","'num_shells', 'num_access_files', 'num_outbound_cmds', 'count' ,'srv_count',\n","'serror_rate', 'srv_serror_rate' ,'rerror_rate' ,'srv_rerror_rate',\n","'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate' ,'dst_host_count',\n","'dst_host_srv_count' ,'dst_host_same_srv_rate' ,'dst_host_diff_srv_rate',\n","'dst_host_same_src_port_rate' ,'dst_host_srv_diff_host_rate',\n","'dst_host_serror_rate' ,'dst_host_srv_serror_rate', 'dst_host_rerror_rate',\n","'dst_host_srv_rerror_rate']\n","\n","class OutcomeCreater(Transformer): # this defines a transformer that creates the outcome column\n","    \n","    def __init__(self):\n","        super().__init__()\n","\n","    def _transform(self, dataset):\n","        def attack_category(attack_type):\n","            if attack_type == 'normal':\n","                return 0\n","            elif attack_type in ['port-Sweep', 'ip-Sweep', 'nmap', 'satan', 'saint', 'mscan']:\n","                return 1  # Probing\n","            elif attack_type in ['neptune', 'smurf', 'pod', 'teardrop', 'land', 'back', 'apache2',\n","                                'udpstorm', 'processtable', 'mail-Bomb']:\n","                return 2  # Dos\n","            elif attack_type in ['buffer-Overflow', 'load-Module', 'perl', 'rootkit', 'xterm',\n","                                'ps', 'sqlattack']:\n","                return 3  # U2R\n","            else:\n","                return 4  # R2L\n","          \n","        # Convert the function to a UDF, specifying IntegerType for output\n","        label_to_multiclasses = udf(attack_category, IntegerType())\n","        output_df = dataset.withColumn('outcome', label_to_multiclasses(col('class'))).drop(\"class\")  \n","        output_df = output_df.withColumn('outcome', col('outcome').cast(DoubleType()))\n","        output_df = output_df.drop('difficulty')\n","        return output_df\n","\n","class FeatureTypeCaster(Transformer): # this transformer will cast the columns as appropriate types  \n","    def __init__(self):\n","        super().__init__()\n","\n","    def _transform(self, dataset):\n","        output_df = dataset\n","        for col_name in binary_cols + continuous_cols:\n","            output_df = output_df.withColumn(col_name,col(col_name).cast(DoubleType()))\n","\n","        return output_df\n","    \n","class ColumnDropper(Transformer): # this transformer drops unnecessary columns\n","    def __init__(self, columns_to_drop = None):\n","        super().__init__()\n","        self.columns_to_drop=columns_to_drop\n","    def _transform(self, dataset):\n","        output_df = dataset\n","        for col_name in self.columns_to_drop:\n","            output_df = output_df.drop(col_name)\n","        return output_df\n","    \n","def get_preprocess_muticlass_pipeline():\n","    # Stage where columns are casted as appropriate types\n","    stage_typecaster = FeatureTypeCaster()\n","\n","    # Stage where nominal columns are transformed to index columns using StringIndexer\n","    nominal_id_cols = [x+\"_index\" for x in nominal_cols]\n","    nominal_onehot_cols = [x+\"_encoded\" for x in nominal_cols]\n","    stage_nominal_indexer = StringIndexer(inputCols = nominal_cols, outputCols = nominal_id_cols )\n","\n","    # Stage where the index columns are further transformed using OneHotEncoder\n","    stage_nominal_onehot_encoder = OneHotEncoder(inputCols=nominal_id_cols, outputCols=nominal_onehot_cols)\n","\n","    # Stage where all relevant features are assembled into a vector (and dropping a few)\n","    feature_cols = continuous_cols+binary_cols+nominal_onehot_cols\n","    corelated_cols_to_remove = [\"dst_host_serror_rate\",\"srv_serror_rate\",\"dst_host_srv_serror_rate\",\n","                     \"srv_rerror_rate\",\"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\"]\n","    for col_name in corelated_cols_to_remove:\n","        feature_cols.remove(col_name)\n","    stage_vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"vectorized_features\")\n","\n","    # Stage where we scale the columns\n","    stage_scaler = StandardScaler(inputCol= 'vectorized_features', outputCol= 'features')\n","    \n","\n","    # Stage for creating the outcome column representing whether there is normal, DOS, R2L, U2R, probing.\n","    stage_outcome = OutcomeCreater()\n","\n","    # Removing all unnecessary columbs, only keeping the 'features' and 'outcome' columns\n","    stage_column_dropper = ColumnDropper(columns_to_drop = nominal_cols+nominal_id_cols+\n","        nominal_onehot_cols+ binary_cols + continuous_cols + ['vectorized_features'])\n","    \n","    # Connect the columns into a pipeline\n","    pipeline = Pipeline(stages=[stage_typecaster,stage_nominal_indexer,stage_nominal_onehot_encoder,\n","        stage_vector_assembler,stage_scaler,stage_outcome,stage_column_dropper])\n","    return pipeline "]},{"cell_type":"code","execution_count":4,"id":"62e53267","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["24/10/14 22:21:24 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n","[Stage 8:>                                                          (0 + 1) / 1]\r"]},{"name":"stdout","output_type":"stream","text":["-RECORD 0------------------------\n"," features | (113,[1,13,14,17,... \n"," outcome  | 0.0                  \n","-RECORD 1------------------------\n"," features | (113,[1,13,14,17,... \n"," outcome  | 0.0                  \n","-RECORD 2------------------------\n"," features | (113,[13,14,15,17... \n"," outcome  | 2.0                  \n","only showing top 3 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["preprocess_multi_class_pipeline = get_preprocess_muticlass_pipeline()\n","preprocess_multi_class = preprocess_multi_class_pipeline.fit(nslkdd_raw)\n","\n","# Trandform train dataset\n","nslkdd_multi = preprocess_multi_class.transform(nslkdd_raw)\n","nslkdd_multi.show(3, vertical=True)"]},{"cell_type":"code","execution_count":5,"id":"7939e6ea","metadata":{"scrolled":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\r","[Stage 9:>                                                          (0 + 1) / 1]\r"]},{"name":"stdout","output_type":"stream","text":["-RECORD 0------------------------\n"," features | (113,[13,14,16,17... \n"," outcome  | 2.0                  \n","-RECORD 1------------------------\n"," features | (113,[13,14,16,17... \n"," outcome  | 2.0                  \n","-RECORD 2------------------------\n"," features | (113,[0,1,13,14,1... \n"," outcome  | 0.0                  \n","only showing top 3 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["# Trandform test datset \n","nslkdd_test_multi = preprocess_multi_class.transform(nslkdd_test_raw)\n","nslkdd_test_multi.show(3, vertical=True)"]},{"cell_type":"markdown","id":"81dacd6e","metadata":{},"source":["#### Machine Learning process"]},{"cell_type":"code","execution_count":7,"id":"8ac85bec","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Train Accuracy : 96.98%\n","Test Accuracy : 71.9%\n"]}],"source":["# Create a logistic regression model\n","lr = LogisticRegression(featuresCol = 'features', \n","                             labelCol = 'outcome', \n","                             maxIter=10)\n","\n","# Fit the model\n","lrModel = lr.fit(nslkdd_multi)\n","\n","# Calculate a train accuracy\n","predictions_train = lrModel.transform(nslkdd_multi)\n","accuracy_train = (predictions_train.filter(predictions_train.outcome == predictions_train.prediction)\n","    .count() / float(predictions_train.count()))\n","# Calculate a test accuracy\n","lr_predictions_test = lrModel.transform(nslkdd_test_multi)\n","accuracy_test = (lr_predictions_test.filter(lr_predictions_test.outcome == lr_predictions_test.prediction)\n","    .count() / float(lr_predictions_test.count()))\n","\n","print(f\"Train Accuracy : {np.round(accuracy_train*100,2)}%\")\n","print(f\"Test Accuracy : {np.round(accuracy_test*100,2)}%\")"]},{"cell_type":"markdown","id":"371d3c1c","metadata":{},"source":["#### Print the number of partitions of Dataframe"]},{"cell_type":"code","execution_count":6,"id":"df72ba36","metadata":{},"outputs":[{"data":{"text/plain":["DataFrame[features: vector, outcome: double]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["nslkdd_multi.cache()\n","nslkdd_test_multi.cache()"]},{"cell_type":"code","execution_count":9,"id":"a4645e0c","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of partitions: 2\n"]}],"source":["num_partitions = nslkdd_multi.rdd.getNumPartitions()\n","print(f\"Number of partitions: {num_partitions}\")"]},{"cell_type":"code","execution_count":10,"id":"8e691bdb","metadata":{},"outputs":[],"source":["spark.stop()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":5}